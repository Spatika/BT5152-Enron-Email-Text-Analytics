### Libraries
```{r}
#library(formattable) # output is easier to read an well formatted
library(ggplot2)
library(tm)
library(ROCR)
library(parallel)
library(doParallel)
library(tm)
library(caret)
library(plyr)
library(dplyr)
library(stringr)
library(jsonlite)
library(topicmodels)
library(tidytext)
library(tidyr)
library(tibble)
library(tm.lexicon.GeneralInquirer)
```
### Start cluster
```{r}
cl = makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
tm_parLapply_engine(cl)
```

### Read Datasets
```{r}
emails <- readxl::read_xlsx("email_data.xlsx")
enron <- read.csv("enron_cleaned.csv", stringsAsFactors = FALSE)
merged <- merge(emails, enron, by.x = "X-From", by.y = "X")
merged <- merged[,-2]
merged$poi <- as.factor(merged$poi)
```

## Data Exploration
```{r}
payment_data <- c('salary',
                'bonus',
                'long_term_incentive',
                'deferred_income',
                'deferral_payments',
                'loan_advances',
                'other',
                'expenses',                
                'director_fees', 
                'total_payments')

stock_data <- c('exercised_stock_options',
              'restricted_stock',
              'restricted_stock_deferred',
              'total_stock_value')

email_data <- c("content",
              "to_messages",
              'from_messages',
              'from_poi_to_this_person',
              'from_this_person_to_poi',
              'shared_receipt_with_poi')

features_list <- paste(c('poi'), payment_data , stock_data ,email_data)

nrow(merged)
sum(as.numeric(merged$poi)-1)
summary(merged)
```

## TF-IDF/Dictionary Approach Features
```{r}
email_corpus <- VCorpus(VectorSource(merged$content))
email_corpus <- tm_map(email_corpus, content_transformer(tolower))
email_corpus <- tm_map(email_corpus, content_transformer(gsub), pattern="\\W",replace=" ") # remove emojis
email_corpus <- tm_map(email_corpus, removeNumbers) # remove numbers
email_corpus <- tm_map(email_corpus, removeWords, stopwords()) # remove stop words
email_corpus <- tm_map(email_corpus, removePunctuation) # remove punctuation
email_corpus <- tm_map(email_corpus, stemDocument)
conv2space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
email_corpus <- tm_map(email_corpus, conv2space, "/")
email_corpus <- tm_map(email_corpus, conv2space, "@")
email_corpus <- tm_map(email_corpus, conv2space, "!")
email_corpus <- tm_map(email_corpus, stripWhitespace) # eliminate unneeded whitespace

dtm_control <- list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
email_dtm <- DocumentTermMatrix(email_corpus, control = dtm_control)

# neg score, placeholder for some other dictionary approach
tf_idf_features <- tm_term_score(email_dtm, terms_in_General_Inquirer_categories("Negativ")) 

# tf_idf_features <- tbl_df(as.matrix(email_dtm))
# nzv_columns <- nearZeroVar(tf_idf_features)
# tf_idf_features <- tf_idf_features[, -nzv_columns]
```

## Document-Topic Distributions as Features
```{r}
#need weightTf matrix, not Tf-IDF
dtm_control2 <- list(weighting = weightTf)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

dtm_control2$dictionary <- findFreqTerms(email_dtm2, lowfreq = 100)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

# Remove docs which all frequencies are zeros (side effect of high frequency terms filtering during pre-processing)

row_totals <- apply(email_dtm2 , 1, sum)
email_dtm2 <- email_dtm2[apply(email_dtm2 , 1, sum) > 0,]

heldout_index <- sample(email_dtm2$nrow*0.1)

ks <- seq(5, 10, 1)

topic_models <- parSapply(cl = cl, ks, function(k, data) topicmodels::LDA(data, k = k, method = "Gibbs"), data = email_dtm2[-heldout_index,])

perplexities <- parSapply(cl = cl, topic_models, function(m, data) topicmodels::perplexity(m, data, use_theta = TRUE, estimate_theta = TRUE), data = email_dtm2[heldout_index,])

optimal_idx <- which.min(perplexities)

# get document-topic dist for all of email_dtm2 with posterior, then use those as features, add to features_df # # --> need to preserve names
docs_topics <- posterior(topic_models[[optimal_idx]], email_dtm2)
document_topic_dist <- docs_topics[["topics"]]

colnames(document_topic_dist) <- paste("Topic", colnames(document_topic_dist), sep = "_")

# Make sure to stop cluster *plus* insert serial backend
stopCluster(cl); print("Cluster stopped.")
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
```

## Feature Construction Method 3
```{r}

```

## Scale numerical variables
```{r}
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

denormalize <- function(x, min_x, max_x) { 
  return(x * (max_x - min_x) + min_x) 
}

combined_features_df <- merged[,-c(1,2,3)]
combined_features_df <- combined_features_df %>% mutate_if(is.numeric, normalize)
```

## Combine Text Features to original feature list
```{r}
colnames(combined_features_df) <- paste("Main", colnames(combined_features_df), sep = "_")
combined_features_df <- cbind(combined_features_df, tf_idf_features, document_topic_dist)
```

## 3A - Split into training and validation and perform scaling
```{r}
train_idx <- sample(nrow(combined_features_df) * 0.60)
train_features_df <- combined_features_df[train_idx,]
test_features_df <- combined_features_df[-train_idx,]
train_poi <- as.data.frame(merged[train_idx,"poi"])
colnames(train_poi) <- c("poi")
test_poi <- as.data.frame(merged[-train_idx,"poi"])
colnames(test_poi) <- c("poi")
```

## 3B - Baseline C5.0 Model
```{r}
library(C50)

# common control for all the baseline methods in 3B and 3C
control <- trainControl(method = "cv", number = 5, sampling = "smote", classProbs = TRUE, summaryFunction = twoClassSummary)

# winnow - feature selection or not, trials - number of boosting iterations (more than 5 causing some warnings)
grid <- expand.grid(.model = "tree", .winnow = c(TRUE,FALSE), .trials = 1:5) 

model_C50 <- train(poi ~ ., data = cbind(train_features_df, train_poi), method = "C5.0", 
                   metric = "ROC", trControl = control, 
                   preProcess = c("scale", "center"), 
                   tuneGrid = grid)

train_pred_C50 <- predict(model_C50, train_features_df) #predict
mean(train_pred_C50 == train_poi$poi) # calculate accuracy 0.893

test_pred_C50 <- predict(model_C50, test_features_df) #predict
mean(test_pred_C50 == test_poi$poi) # calculate accuracy 0.8039

# PRECISION and RECALL
test_cm_C50 <- table(predict(model_C50, test_features_df), test_poi$poi)
confusionMatrix(test_cm_C50, mode = 'everything') # acc 0.8039, pre 0.9286, recall 0.8478

# AUC
test_pred_C50 <- ROCR::prediction(as.numeric(predict(model_C50, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_C50, "auc")@y.values) # 0.6239 auc
```

## 3B - Baseline k-NN Model
```{r}
library(caret)
library(tidyverse)

model_knn <- train(poi ~., data = cbind(train_features_df, train_poi), method="knn", trControl = control,
                   tuneGrid = expand.grid(k = c(3, 7, 11)),
                   preProcess = c("scale", "center"),
                   metric = "ROC")

# Training accuracy 0.82
pred_knn_tr <- predict(model_knn, newdata = train_features_df)
mean(pred_knn_tr == train_poi$poi)

# Testing accuracy 0.82
pred_knn <- predict(model_knn,newdata = test_features_df)
mean(pred_knn == test_poi$poi)
cm = confusionMatrix(pred_knn, test_poi$poi)

cm$byClass["Precision"] # 0.911
cm$byClass["Recall"] # 0.89

# AUC
test_pred_knn <- ROCR::prediction(as.numeric(pred_knn), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_knn, "auc")@y.values) # 0.545 auc

```

## 3B - Baseline RPart Model
```{r}
library(rpart)

control <- trainControl(method = "cv", number = 5, sampling = "smote", classProbs= TRUE, summaryFunction = twoClassSummary)

grid <- expand.grid(.cp = c(0.005, 0.01, 0.02, 0.05, 0.1))

rpart_model_smote <- train(poi ~ ., data = cbind(train_features_df, train_poi), 
                           method = "rpart", metric = "ROC", 
                           trControl = control, preProcess = c("scale", "center"), 
                           tuneGrid = grid)

rpart_model_smote$bestTune

# PRECISION and RECALL
test_cm_rpart <- table(predict(rpart_model_smote, test_features_df), test_poi$poi)
confusionMatrix(test_cm_rpart, mode = 'everything') # acc 0.7255, pre 0.9444, recall 0.7391

# AUC
test_pred_rpart <- ROCR::prediction(as.numeric(predict(rpart_model_smote, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_rpart, "auc")@y.values) # 0.669 auc

```

## 3B - Baseline NNet Model
```{r}
library(nnet)
train2 = cbind(train_features_df,train_poi)
test2 = cbind(test_features_df,test_poi)

train2[is.na(train2)] = 0
test2[is.na(test2)] = 0

nn2 = train(poi ~., method = 'nnet', data = train2)
summary(nn2)

nn2_pred = predict(nn2, train_features_df)
mean(nn2_pred == train_poi$poi)
confusionMatrix(nn2_pred, train_poi$poi) #Training accuracy 
# test data pred
test_pred1 = predict(nn2, test_features_df)
mean(test_pred1 == test_poi$poi)
confusionMatrix(test_pred1, test_poi$poi) #Testing accuracy 

# tuning the model
cvCtrl = trainControl(method="cv", number = 5, sampling = 'smote')
grid = expand.grid(size = 2, decay = 0)
nn3 = train(poi ~., method = 'nnet', data = train2, trControl = cvCtrl, tuneGrid = grid,preProcess = c("scale", "center"), tuneLength = 5)
nn3_pred = predict(nn3, train2)
confusionMatrix(nn3_pred, train_poi$poi) #Training accuracy 
# test data pred 
test_pred2 = predict(nn3, test2)
confusionMatrix(test_pred2, test_poi$poi) #Testing accuracy

#Precision-recall testing
train_labels = train_poi$poi
test_labels = test_poi$poi
#Precision-recall curve for model nn2
pred = ROCR::prediction(as.numeric(test_pred1), as.numeric(test_labels))
perf1 = performance(pred, "prec", "rec")
plot(perf1) # Very low  precisions
#Precision-recall curve for model nn3
pred2 = ROCR::prediction(as.numeric(test_pred2), as.numeric(test_labels))
perf2 = performance(pred2, "prec", "rec")
plot(perf2) # Very low  precisions
```


## 3B - Baseline Support Vector Machine Model
```{r}
library(caret)
library(e1071)
library(DMwR)

# control <-  trainControl(method="cv", number = 5, sampling = 'smote')- using same control as above
model_svm  <- train(factor(poi) ~., data = cbind(train_features_df, train_poi),
                    metric = "ROC", method = "svmLinear", 
                    trControl = control, preProcess = c("scale", "center"))

# Variable(s) `' constant. Cannot scale data.???

# training accuracy 0.933
pred_svm_te <- predict(model_svm, train_features_df)
mean(pred_svm_tr == train_poi$poi)

# testing accuracy 0.77
pred_svm <- predict(model.svm, test_features_df)
mean(pred_svm == test_poi$poi)
cm = confusionMatrix(pred_svm, test_poi$poi)

cm$byClass["Precision"] # 0.954
cm$byClass["Recall"] # 0.913

# AUC
test_pred_svm <- ROCR::prediction(as.numeric(pred_svm), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_svm, "auc")@y.values) # 0.756 auc

```

## Random Forest SMOTE - improves Recall rate to 85.7%
```{r}
rf_model <- train(poi ~ ., data = cbind(train_features_df,train_poi), method = "rf", trControl = trainControl(method = "cv", number = 5))

test_cm <- table(predict(rf_model, test_features_df), test_poi$poi)
confusionMatrix(test_cm, mode = "everything")

rf_model_smote <- train(poi ~ ., data = cbind(train_features_df,train_poi), 
                        method = "rf", trControl = control, metric = "ROC")
test_cm <- table(predict(rf_model_smote, test_features_df), test_poi$poi)
confusionMatrix(test_cm, mode = "everything")
```

## 3C - Random Forest using Ranger
```{r}
ctrl <- trainControl(method = 'cv', number = 3,verboseIter = TRUE)
ranger_mod <- train(x = train_features_df, 
                y = train_poi$poi, 
                method = "ranger",
                trControl = ctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(train_features_df)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))

train_pred <- predict(ranger_mod, train_features_df)
train_cm = as.matrix(table(Actual=train_poi$po, Predicted=train_pred))
train_cm

test_pred <- predict(ranger_mod, test_features_df)
test_cm = as.matrix(table(Actual=test_poi$poi, Predicted=test_pred))
test_cm
```

## 3C - Boosting
```{r}
# xgboost with caret train

```

## 3C - Ensemble Stacking
```{r}
library(caretEnsemble)
library(caret)

folds <- createFolds(train_poi$poi, k = 5)

# summaryFunction so that we can use ROC as our metric for primary model selection later
control <- trainControl(method='repeatedcv', number=5, repeats=3, index=folds, savePredictions='final', classProbs=TRUE, summaryFunction=twoClassSummary)

algos <- c('rf','nnet')

models <- caretList(train_poi$poi ~ ., data=cbind(train_features_df, train_poi$poi), trControl=control, metric='ROC', methodList=algos)

models_perf <- resamples(models)
summary(models_perf)
modelCor(models_perf)
```

### 3D - Features Engineering
```{r}
merged$to_poi_ratio = merged$from_poi_to_this_person / merged$to_messages
merged$from_poi_ratio = merged$from_this_person_to_poi / merged$from_messages
merged$shared_poi_ratio = merged$shared_receipt_with_poi / merged$to_messages
```

### 3B and 3C models again with engineered features
```{r}

```
