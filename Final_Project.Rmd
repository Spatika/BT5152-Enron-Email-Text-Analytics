### Libraries
```{r}
library(ggplot2)
library(tm)
library(ROCR)
library(parallel)
library(doParallel)
library(C50)
library(caret)
library(rpart)
library(plyr)
library(dplyr)
library(stringr)
library(jsonlite)
library(topicmodels)
library(tidytext)
library(tidyr)
library(tibble)
library(tidyverse)
library(tm.lexicon.GeneralInquirer)
```
### Start cluster
```{r}
cl = makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
tm_parLapply_engine(cl)
```

### Read Datasets
```{r}
emails <- readxl::read_xlsx("email_data.xlsx")
enron <- read.csv("enron_cleaned.csv", stringsAsFactors = FALSE)
merged <- merge(emails, enron, by.x = "X-From", by.y = "X")
merged <- merged[,-2]
merged$poi <- as.factor(merged$poi)
```

## Data Exploration
```{r}
payment_data <- c('salary',
                'bonus',
                'long_term_incentive',
                'deferred_income',
                'deferral_payments',
                'loan_advances',
                'other',
                'expenses',                
                'director_fees', 
                'total_payments')

stock_data <- c('exercised_stock_options',
              'restricted_stock',
              'restricted_stock_deferred',
              'total_stock_value')

email_data <- c("content",
              "to_messages",
              'from_messages',
              'from_poi_to_this_person',
              'from_this_person_to_poi',
              'shared_receipt_with_poi')

features_list <- paste(c('poi'), payment_data , stock_data ,email_data)

nrow(merged)
sum(as.numeric(merged$poi)-1)
summary(merged)
```

## TF-IDF/Dictionary Approach Features
```{r}
email_corpus <- VCorpus(VectorSource(merged$content))
email_corpus <- tm_map(email_corpus, content_transformer(tolower))
email_corpus <- tm_map(email_corpus, content_transformer(gsub), pattern="\\W",replace=" ") # remove emojis
email_corpus <- tm_map(email_corpus, removeNumbers) # remove numbers
email_corpus <- tm_map(email_corpus, removeWords, stopwords("SMART")) # remove stop words
email_corpus <- tm_map(email_corpus, removePunctuation) # remove punctuation
email_corpus <- tm_map(email_corpus, stemDocument)
conv2space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
email_corpus <- tm_map(email_corpus, conv2space, "/")
email_corpus <- tm_map(email_corpus, conv2space, "@")
email_corpus <- tm_map(email_corpus, conv2space, "!")
email_corpus <- tm_map(email_corpus, stripWhitespace) # eliminate unneeded whitespace

dtm_control <- list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
email_dtm <- DocumentTermMatrix(email_corpus, control = dtm_control)

# neg score 
dict_features <- tm_term_score(email_dtm, terms_in_General_Inquirer_categories("Negativ")) 
```

## Document-Topic Distributions as Features
```{r}
# need weightTf matrix, not Tf-IDF
dtm_control2 <- list(weighting = weightTf)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

dtm_control2$dictionary <- findFreqTerms(email_dtm2, lowfreq = 100)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

# Remove docs which all frequencies are zeros (side effect of high frequency terms filtering during pre-processing)
row_totals <- apply(email_dtm2 , 1, sum)
email_dtm2 <- email_dtm2[apply(email_dtm2 , 1, sum) > 0,]

heldout_index <- sample(email_dtm2$nrow*0.1)

ks <- seq(2, 6, 1)

topic_models <- parSapply(cl = cl, ks, function(k, data) topicmodels::LDA(data, k = k, method = "Gibbs", control = list(iter = 3000, seed = 5152)), data = email_dtm2[-heldout_index,])

perplexities <- parSapply(cl = cl, topic_models, function(m, data) topicmodels::perplexity(m, data, use_theta = TRUE, estimate_theta = TRUE), data = email_dtm2[heldout_index,])

optimal_idx <- which.min(perplexities)

# get document-topic dist for all of email_dtm2 with posterior, then use those as features, add to features_df # # --> need to preserve names
docs_topics <- posterior(topic_models[[optimal_idx]], email_dtm2)
document_topic_dist <- docs_topics[["topics"]]

colnames(document_topic_dist) <- paste("Topic", colnames(document_topic_dist), sep = "_")

# Make sure to stop cluster *plus* insert serial backend
stopCluster(cl); print("Cluster stopped.")
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
```
## Topic-Term Visualization
```{r}
lda <- topic_models[[optimal_idx]]

lda_td <- tidy(lda)

top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms

# visualization
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```
## Feature from Python Word2Vec Clustering (Code to create this feature is in Python file)
```{r}
word2vec_cluster_labels = read.csv("cluster_labels.csv", stringsAsFactors = FALSE, header = F)
```
## Create Latent Semantic Analyis Model on TF-IDF Vectors and use SVD to reduce Dimension then Feed into Features for Modelling (Code to create this feature is in Python file)
```{r}
lsa_features = as.matrix(read.csv("LSA_SVD.csv", stringsAsFactors = FALSE, header = F))
```
## Scale numerical variables
```{r}
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

denormalize <- function(x, min_x, max_x) { 
  return(x * (max_x - min_x) + min_x) 
}

combined_features_df <- merged[,-c(1,2,3)]
combined_features_df <- combined_features_df %>% mutate_if(is.numeric, normalize)
```

## Combine Text Features to original feature list
```{r}
colnames(combined_features_df) <- paste("Main", colnames(combined_features_df), sep = "_")
combined_features_df <- cbind(combined_features_df, dict_features, document_topic_dist, word2vec_cluster_labels,lsa_features)

# remove features with near zero variance - Main_loan_advances and Main_director_fees and Main_restricted_stock_deferred
combined_features_df <- select(combined_features_df, -Main_loan_advances, -Main_director_fees, -Main_restricted_stock_deferred)
```

## 3A - Split into training and validation and perform scaling
```{r}
train_idx <- createDataPartition(merged$poi, p = 0.7, list = FALSE)

# to see if train and test will have balanced distribution of POI
###################################
train_merge <- merged[train_idx,]
test_merge <- merged[-train_idx,]

table(train_merge$poi)
table(test_merge$poi)
###################################

train_features_df <- combined_features_df[train_idx,]
test_features_df <- combined_features_df[-train_idx,]

train_poi <- as.data.frame(merged[train_idx,"poi"])
colnames(train_poi) <- c("poi")
test_poi <- as.data.frame(merged[-train_idx,"poi"])
colnames(test_poi) <- c("poi")
```

## 3B - Baseline Cross-validated C5.0 Model with Smote OverSampling
```{r}
# common control for all the baseline methods in 3B and 3C
control <- trainControl(method = "repeatedcv", number = 5, repeats = 2, sampling = "smote", classProbs = TRUE, summaryFunction = twoClassSummary)

# winnow - feature selection or not, trials - number of boosting iterations
grid <- expand.grid(.model = "tree", .winnow = c(TRUE,FALSE), .trials = 1:5) 

set.seed(5152)
model_C50 <- train(poi ~ ., data = cbind(train_features_df, train_poi), method = "C5.0", 
                   metric = "ROC", trControl = control, 
                   preProcess = c("scale", "center"), 
                   tuneGrid = grid)

train_pred_C50 <- predict(model_C50, train_features_df) #predict
mean(train_pred_C50 == train_poi$poi) # calculate accuracy 0.92

test_pred_C50 <- predict(model_C50, test_features_df) #predict
mean(test_pred_C50 == test_poi$poi) # calculate accuracy 0.784

# PRECISION and RECALL
train_cm_C50 <- table(train_pred_C50, train_poi$poi)
confusionMatrix(train_cm_C50, mode = 'everything') # acc 0.784, pre 0.9487, recall 0.8043

test_cm_C50 <- table(test_pred_C50, test_poi$poi)
confusionMatrix(test_cm_C50, mode = 'everything') # acc 0.784, pre 0.9487, recall 0.8043

# AUC
test_pred_C50 <- ROCR::prediction(as.numeric(test_pred_C50), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_C50, "auc")@y.values) # 0.702 auc
```

## 3B - Baseline Cross-validated k-NN Model with SMOTE sampling
```{r}
set.seed(5152)
model_knn <- train(poi ~., data = cbind(train_features_df, train_poi), method="knn", trControl = control,
                   tuneGrid = expand.grid(k = c(3, 7, 11)),
                   preProcess = c("scale", "center"),
                   metric = "ROC")

# Training accuracy 
pred_knn_tr <- predict(model_knn, newdata = train_features_df)
mean(pred_knn_tr == train_poi$poi)

# Testing accuracy
pred_knn <- predict(model_knn,newdata = test_features_df)
mean(pred_knn == test_poi$poi)

# PRECISION and RECALL
train_cm = confusionMatrix(pred_knn_tr, train_poi$poi)
train_cm$byClass["Precision"] # 0.93
train_cm$byClass["Recall"] # 0.869

cm = confusionMatrix(pred_knn, test_poi$poi)
cm$byClass["Precision"] # 0.93
cm$byClass["Recall"] # 0.869
cm$byClass["F1"]

# AUC
test_pred_knn <- ROCR::prediction(as.numeric(pred_knn), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_knn, "auc")@y.values) # 0.59 auc

```

## 3B - Baseline Cross-Validated RPart Model with SMOTE Sampling
```{r}
control <- trainControl(method = "cv", number = 5, sampling = "smote", classProbs= TRUE, summaryFunction = twoClassSummary)

grid <- expand.grid(.cp = c(0.005, 0.01, 0.02, 0.05, 0.1))

set.seed(5152)
rpart_model_smote <- train(poi ~ ., data = cbind(train_features_df, train_poi), 
                           method = "rpart", metric = "ROC", 
                           trControl = control, preProcess = c("scale", "center"), 
                           tuneGrid = grid)

rpart_model_smote$bestTune

# PRECISION and RECALL
train_cm_rpart <- table(predict(rpart_model_smote, train_features_df), train_poi$poi)
confusionMatrix(train_cm_rpart, mode = 'everything') # acc 0.823, pre 0.951, recall 0.847

test_cm_rpart <- table(predict(rpart_model_smote, test_features_df), test_poi$poi)
confusionMatrix(test_cm_rpart, mode = 'everything') # acc 0.823, pre 0.951, recall 0.847

# AUC
test_pred_rpart <- ROCR::prediction(as.numeric(predict(rpart_model_smote, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_rpart, "auc")@y.values) # 0.723 auc

```

## 3B - Baseline Cross-Validated NNet Model with SMOTE sampling
```{r}
library(nnet)
train_nnet = cbind(train_features_df,train_poi)
test_nnet = cbind(test_features_df,test_poi)

train_nnet[is.na(train_nnet)] = 0
test_nnet[is.na(test_nnet)] = 0

grid = expand.grid(.size = c(1,2,3), .decay = c(0, 1, 2))

set.seed(5152)
model_nn = train(poi ~., method = 'nnet', data = train_nnet, metric = "ROC",
            trControl = control, tuneGrid = grid, preProcess = c("scale", "center"))

model_nn$bestTune
# train data pred
nn_pred_train = predict(model_nn, train_nnet)
confusionMatrix(nn_pred_train, train_poi$poi, mode = 'everything') # training accuracy 

# test data pred 
nn_pred_test = predict(model_nn, test_nnet)
confusionMatrix(nn_pred_test, test_poi$poi, mode = 'everything') # testing accuracy - 0.78, prec - 0.926, recall - 0.826

# AUC
test_pred_nn <- ROCR::prediction(as.numeric(predict(model_nn, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_nn, "auc")@y.values) # 0.602 auc

```


## 3B - Baseline Cross-Validated Support Vector Machine Model with Smote Sampling
```{r}
library(caret)
library(e1071)
library(DMwR)

set.seed(5152)

#tuning with cost parameter
model_svm_c <- train(factor(poi) ~., data = cbind(train_features_df, train_poi),
                    metric = "ROC", method = "svmLinear", 
                    trControl = control, tuneGrid = expand.grid(.C= c(1,2,0.5)))

model_svm_c$bestTune
 
# training accuracy 0.94
pred_svm_tr <- predict(model_svm_c, train_features_df)
mean(pred_svm_tr == train_poi$poi)  

# testing accuracy 0.83
pred_svm <- predict(model_svm_c, test_features_df)
mean(pred_svm == test_poi$poi)

#PRECISION and RECALL
train_cm = confusionMatrix(pred_svm_tr, train_poi$poi)

train_cm$byClass["Precision"] 
train_cm$byClass["Recall"]

cm = confusionMatrix(pred_svm, test_poi$poi)

cm$byClass["Precision"] 
cm$byClass["Recall"]
cm$byClass["F1"] 

# AUC
test_pred_svm <- ROCR::prediction(as.numeric(pred_svm), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_svm, "auc")@y.values) # 0.737 auc

```

## 3C - Ensemble : Random Forest - improves recall to 0.934
```{r}
set.seed(5152)
rf_model_smote <- train(poi ~ ., data = cbind(train_features_df,train_poi), 
                        method = "rf", trControl = control, metric = "ROC",
                        preProcess = c("scale", "center"))

test_cm <- table(predict(rf_model_smote, test_features_df), test_poi$poi)
confusionMatrix(test_cm, mode = "everything") # acc 0.8824, prec 0.9556, recall 0.9348

# AUC
test_pred_rf <- ROCR::prediction(as.numeric(predict(rf_model_smote, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_rf, "auc")@y.values) # 0.7 auc
```

## 3C - Ensemble :Random Forest using Ranger
```{r}
# ctrl <- trainControl(method = 'cv', number = 3, verboseIter = TRUE) -- using common control instead from C50

ranger_model <- train(x = train_features_df, 
                y = train_poi$poi, 
                method = "ranger",
                trControl = control,
                metric = "ROC",
                preProcess = c("scale", "center"),
                tuneGrid = data.frame(mtry = floor(sqrt(dim(train_features_df)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))

train_pred <- predict(ranger_model, train_features_df)
train_cm = as.matrix(table(Actual=train_poi$po, Predicted=train_pred))
train_cm

test_pred <- predict(ranger_model, test_features_df)
test_cm = as.matrix(table(Actual=test_poi$poi, Predicted=test_pred))
test_cm
```

## 3C - Ensemble :Boosting
```{r}
# xgboost with caret train

grid <- expand.grid(.nrounds=c(25, 50, 100, 200, 500),.eta=c(0.005, 0.01, 0.05, 0.1),.gamma=c(0.1,0.5, 1, 1.5),
                     .max_depth=c(2,3,4,5),.colsample_bytree=c(0.6, 0.8, 1),
                     .subsample=c(0.5, 0.75, 1),.min_child_weight=c(1, 2, 3, 5))


set.seed(5152)

starttime <- Sys.time()
model_xgb <- train(poi ~ ., data = cbind(train_features_df,train_poi), 
                   method = "xgbTree", trControl = control, 
                   tuneGrid = grid, metric = "ROC",
                  preProcess = c("scale", "center"))
endtime <-Sys.time()

print(endtime-starttime)

print(varImp(model_xgb))
plot(varImp(model_xgb))

# ACCURACY, PRECISION and RECALL
test_cm_xgb <- table(predict(rpart_model_smote, test_features_df), test_poi$poi)
confusionMatrix(test_cm_xgb, mode = 'everything') # acc 0.8235, pre 0.951, recall 0.8478

# AUC
test_pred_xgb <- ROCR::prediction(as.numeric(predict(model_xgb, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_xgb, "auc")@y.values) # 0.723 auc
```

## 3C - Ensemble Stacking
```{r}

library(caretEnsemble)
library(caret)

TL=list(
   m1=caretModelSpec(method='C5.0'),
   m2=caretModelSpec(method='rpart'),
   m3=caretModelSpec(method='ranger'))

set.seed(5152)
folds = createFolds(train_poi$poi, k = 5)

set.seed(5152)
stack_control = trainControl(method='repeatedcv', number=5, repeats=3, index=folds, savePredictions='final', classProbs=TRUE, summaryFunction=twoClassSummary)

models = caretList(poi ~ ., data=train2, metric = 'ROC', trControl=stack_control, tuneList = TL)

results = resamples(models)
summary(results)

stack.glm = caretStack(models, method="glm", metric="ROC", trControl=stack_control)
print(stack.glm)
  
stack_pred = predict(stack.glm, train_features_df)
confusionMatrix(stack_pred,train_poi$poi, mode = 'everything')

stack_pred_test = predict(stack.glm, test_features_df)
confusionMatrix(stack_pred_test,test_poi$poi, mode = 'everything')
```

### 3D - Feature Engineering

```{r}
combined_features_df2 <- combined_features_df

# new features 
combined_features_df2$to_poi_ratio = merged$from_poi_to_this_person / merged$to_messages
combined_features_df2$from_poi_ratio = merged$from_this_person_to_poi / merged$from_messages
combined_features_df2$shared_poi_ratio = merged$shared_receipt_with_poi / merged$to_messages

#financial features
combined_features_df2$bonus_to_salary = merged$bonus / merged$salary
combined_features_df2$bonus_to_total = merged$bonus / merged$total_payments
combined_features_df2$bonus_to_salary[is.na(combined_features_df2$bonus_to_salary)] = 0
combined_features_df2$bonus_to_total[is.na(combined_features_df2$bonus_to_total)] = 0

# train test split again
train_features_df2 <- combined_features_df2[train_idx,]
test_features_df2 <- combined_features_df2[-train_idx,]
```

Report how much those 3 new features improve the prediction performance based on top algorithms in 3B and 3C. You donâ€™t need to re-run all algorithms in 3B and C, but you need to report the results of at least ONE best algorithm in 3B and ONE best algorithm in 3C to show the benefit of features engineering.

### 3B and 3C models again with engineered features
```{r}
####################
## 3B model - Best Algorithm - SVM ##
####################
# tuning with cost parameter
set.seed(5152)

#tuning with cost parameter
model_svm_c <- train(factor(poi) ~., data = cbind(train_features_df2, train_poi),
                    metric = "ROC", method = "svmLinear", 
                    trControl = control, tuneGrid = expand.grid(.C= c(1,2,0.5)))
 
# training accuracy 0.955
pred_svm_tr <- predict(model_svm_c, train_features_df2)
mean(pred_svm_tr == train_poi$poi)  

# testing accuracy 0.837
pred_svm <- predict(model_svm_c, test_features_df2)
mean(pred_svm == test_poi$poi)
cm = confusionMatrix(pred_svm, test_poi$poi)

cm$byClass["Precision"] # 0.84
cm$byClass["Recall"] # 0.84

# AUC
test_pred_svm <- ROCR::prediction(as.numeric(pred_svm), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_svm, "auc")@y.values) # 0.737 auc

################
## 3C model - ## Best Algorithm - Stacking
################

library(caretEnsemble)
library(caret)

TL=list(
   m1=caretModelSpec(method='C5.0'),
   m2=caretModelSpec(method='rpart'),
   m3=caretModelSpec(method='ranger'))

set.seed(5152)
folds = createFolds(train_poi$poi, k = 5)

set.seed(5152)
stack_control = trainControl(method='repeatedcv', number=5, repeats=3, index=folds, savePredictions='final', classProbs=TRUE, summaryFunction=twoClassSummary)

models = caretList(poi ~ ., data=cbind(train_features_df2, train_poi), metric = 'ROC', trControl=stack_control, tuneList = TL)

results = resamples(models)
summary(results)

stack.glm = caretStack(models, method="glm", metric="ROC", trControl=stack_control)
print(stack.glm)
  
stack_pred = predict(stack.glm, train_features_df)
confusionMatrix(stack_pred,train_poi$poi, mode = 'everything')

stack_pred_test = predict(stack.glm, test_features_df)
confusionMatrix(stack_pred_test,test_poi$poi, mode = 'everything')
```
