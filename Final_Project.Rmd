### Libraries
```{r}
#library(formattable) # output is easier to read an well formatted
library(ggplot2)
library(tm)
library(ROCR)
library(parallel)
library(doParallel)
library(tm)
library(caret)
library(plyr)
library(dplyr)
library(stringr)
library(jsonlite)
library(topicmodels)
library(tidytext)
library(tidyr)
library(tibble)
```
### Start cluster
```{r}
cl = makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
tm_parLapply_engine(cl)
```

### Read Datasets
```{r}
emails <- readxl::read_xlsx("email_data.xlsx")
enron <- read.csv("enron_cleaned.csv", stringsAsFactors = FALSE)
merged <- merge(emails, enron, by.x = "X-From", by.y = "X")
merged <- merged[,-2]
merged$poi <- as.factor(merged$poi)
```

## Data Exploration
```{r}
payment_data <- c('salary',
                'bonus',
                'long_term_incentive',
                'deferred_income',
                'deferral_payments',
                'loan_advances',
                'other',
                'expenses',                
                'director_fees', 
                'total_payments')

stock_data <- c('exercised_stock_options',
              'restricted_stock',
              'restricted_stock_deferred',
              'total_stock_value')

email_data <- c("content",
              "to_messages",
              'from_messages',
              'from_poi_to_this_person',
              'from_this_person_to_poi',
              'shared_receipt_with_poi')

features_list <- paste(c('poi'), payment_data , stock_data ,email_data)

nrow(merged)
sum(as.numeric(merged$poi)-1)
summary(merged)
```

## TF-IDF FEATURES
```{r}
email_corpus <- VCorpus(VectorSource(merged$content))
email_corpus <- tm_map(email_corpus, content_transformer(tolower))
email_corpus <- tm_map(email_corpus, content_transformer(gsub), pattern="\\W",replace=" ") # remove emojis
email_corpus <- tm_map(email_corpus, removeNumbers) # remove numbers
email_corpus <- tm_map(email_corpus, removeWords, stopwords()) # remove stop words
email_corpus <- tm_map(email_corpus, removePunctuation) # remove punctuation
email_corpus <- tm_map(email_corpus, stemDocument)
conv2space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
email_corpus <- tm_map(email_corpus, conv2space, "/")
email_corpus <- tm_map(email_corpus, conv2space, "@")
email_corpus <- tm_map(email_corpus, conv2space, "!")
email_corpus <- tm_map(email_corpus, stripWhitespace) # eliminate unneeded whitespace

dtm_control <- list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
email_dtm <- DocumentTermMatrix(email_corpus, control = dtm_control)
tf_idf_features <- tbl_df(as.matrix(email_dtm))
nzv_columns <- nearZeroVar(tf_idf_features)
tf_idf_features <- tf_idf_features[, -nzv_columns]
```
## LDA FOR DOCUMENT-TOPIC DIST AS FEATURES
```{r}
# need weightTf matrix, not Tf-IDF
dtm_control2 <- list(weighting = weightTf)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

dtm_control2$dictionary <- findFreqTerms(email_dtm2, lowfreq = 100)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

# Remove docs which all frequencies are zeros (side effect of high frequency terms filtering during pre-processing)

row_totals <- apply(email_dtm2 , 1, sum)
email_dtm2 <- email_dtm2[apply(email_dtm2 , 1, sum) > 0,]

heldout_index <- sample(email_dtm2$nrow*0.1)

ks <- seq(5, 10, 1)

topic_models <- parSapply(cl = cl, ks, function(k, data) topicmodels::LDA(data, k = k, method = "Gibbs"), data = email_dtm2[-heldout_index,])

perplexities <- parSapply(cl = cl, topic_models, function(m, data) topicmodels::perplexity(m, data, use_theta = TRUE, estimate_theta = TRUE), data = email_dtm2[heldout_index,])

optimal_idx <- which.min(perplexities)

# get document-topic dist for all of email_dtm2 with posterior, then use those as features, add to features_df # # --> need to preserve names 
docs_topics <- posterior(topic_models[[optimal_idx]], email_dtm2)

document_topic_dist <- docs_topics[["topics"]]

# Make sure to stop cluster *plus* insert serial backend
stopCluster(cl); print("Cluster stopped.")
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
```

## Scale numerical variables and  
```{r}
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

denormalize <- function(x, min_x, max_x) { 
  return(x * (max_x - min_x) + min_x) 
}

combined_features_df <- merged[,-c(1,2,3)]
combined_features_df <- combined_features_df %>% mutate_if(is.numeric, normalize)
```

##Combine Text Features to original feature list
```{r}
colnames(combined_features_df) <- paste("Main", colnames(combined_features_df), sep = "_")
combined_features_df <- cbind(combined_features_df, tf_idf_features)
```

## 3A - Split into training and validation and perform scaling
```{r}
train_idx <- sample(nrow(combined_features_df) * 0.60)
train_features_df <- combined_features_df[train_idx,]
test_features_df <- combined_features_df[-train_idx,]
train_poi <- as.data.frame(merged[train_idx,"poi"])
colnames(train_poi) <- c("poi")
test_poi <- as.data.frame(merged[-train_idx,"poi"])
colnames(test_poi) <- c("poi")
```

## 3B - Baseline C5.0 Model
```{r}
library(C50)
model_tree <- C5.0(poi ~ ., cbind(train_features_df,train_poi)) #Fit model
summary(model_tree) #We can peek into the tree model
train_pred_tree <- predict(model_tree, train_features_df) #predict
mean(train_pred_tree == train_poi$poi) #Calculate accuracy
test_pred_tree <- predict(model_tree, test_features_df) #predict
mean(test_pred_tree == test_poi$poi) #Calculate accuracy

train_cm <- table(train_pred_tree, train_poi$poi)
test_cm <- table(test_pred_tree, test_poi$poi)

precision(train_cm,relevant='True')  
recall(train_cm,relevant='True')
precision(test_cm,relevant='True')   #what % of tuples that the classifier labeled as positive are actually positive
recall(test_cm,relevant='True') #what % of positive tuples did the classifier label as positive
```

## SMOTE - Improves Recall rate to 85.7%
```{r}
model <- train(poi ~ ., data = select(scaled_training, -name), method = "rf", trControl = trainControl(method = "cv", number = 5))
cm <- table(predict(model, select(scaled_test, -name)), scaled_test$poi)
confusionMatrix(cm)
precision(cm,relevant='True')  
recall(cm,relevant='True')
F_meas(cm,relevant='True')

model.smote <- train(poi ~ ., data = select(scaled_training, -name), method = "rf", trControl = trainControl(method = "cv", number = 5, sampling = "smote"))
cm <- table(predict(model.smote, select(scaled_test, -name)), scaled_test$poi)
confusionMatrix(cm)
precision(cm,relevant='True')  
recall(cm,relevant='True')
F_meas(cm,relevant='True')
```

## 3C - Random Forest
```{r}
ctrl <- trainControl(method = 'cv', number = 3,verboseIter = TRUE)
rf_mod <- train(x = features_df, 
                y = as.factor(merged$poi), 
                method = "ranger",
                trControl = ctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(features_df)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))

train_pred <- predict(rf_mod, features_df)
train_cm = as.matrix(table(Actual=merged$poi, Predicted=train_pred))
train_cm
```

## 3C - Ensemble
```{r}
library(caretEnsemble)
library(caret)
folds <- createFolds(scaled_training$poi , k = 5)
#We specify summaryFunction so that we can use ROC as our metric for primary model selection later
control <- trainControl(method='repeatedcv', number=5, repeats=3, index=folds, savePredictions='final', classProbs=TRUE, summaryFunction=twoClassSummary)
algos <- c('rf','nnet')
models <- caretList(poi ~ . , data=scaled_training, trControl=control, metric='ROC', methodList=algos)
models_perf <- resamples(models)
summary(models_perf)
modelCor(models_perf)

train_pred_rf <- predict(models$rf, scaled_training) #predict
mean(train_pred_rf == scaled_training$poi) #Calculate accuracy
test_pred_rf <- predict(model_tree, scaled_test) #predict
mean(test_pred_rf == scaled_test$poi) #Calculate accuracy

train_cm <- table(train_pred_tree, scaled_training$poi)
cm <- table(test_pred_rf, scaled_test$poi)

precision(cm,relevant='True')  
recall(cm,relevant='True')

precision(train_cm,relevant='True') #what % of tuples that the classifier labeled as positive are actually positive
recall(train_cm,relevant='True') #what % of positive tuples did the classifier label as positive
```

### 3D - Features Engineering
```{r}
merged$to_poi_ratio = merged$from_poi_to_this_person / merged$to_messages
merged$from_poi_ratio = merged$from_this_person_to_poi / merged$from_messages
merged$shared_poi_ratio = merged$shared_receipt_with_poi / merged$to_messages
```