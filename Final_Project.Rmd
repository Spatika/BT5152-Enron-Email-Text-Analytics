### Libraries
```{r}
#library(formattable) # output is easier to read an well formatted
library(ggplot2)
library(tm)
library(ROCR)
library(parallel)
library(doParallel)
library(tm)
library(caret)
library(plyr)
library(dplyr)
library(stringr)
library(jsonlite)
library(topicmodels)
library(tidytext)
library(tidyr)
library(tibble)
library(tm.lexicon.GeneralInquirer)
```
### Start cluster
```{r}
cl = makePSOCKcluster(detectCores() - 1)
registerDoParallel(cl)
tm_parLapply_engine(cl)
```

### Read Datasets
```{r}
emails <- readxl::read_xlsx("email_data.xlsx")
enron <- read.csv("enron_cleaned.csv", stringsAsFactors = FALSE)
merged <- merge(emails, enron, by.x = "X-From", by.y = "X")
merged <- merged[,-2]
merged$poi <- as.factor(merged$poi)
```

## Data Exploration
```{r}
payment_data <- c('salary',
                'bonus',
                'long_term_incentive',
                'deferred_income',
                'deferral_payments',
                'loan_advances',
                'other',
                'expenses',                
                'director_fees', 
                'total_payments')

stock_data <- c('exercised_stock_options',
              'restricted_stock',
              'restricted_stock_deferred',
              'total_stock_value')

email_data <- c("content",
              "to_messages",
              'from_messages',
              'from_poi_to_this_person',
              'from_this_person_to_poi',
              'shared_receipt_with_poi')

features_list <- paste(c('poi'), payment_data , stock_data ,email_data)

nrow(merged)
sum(as.numeric(merged$poi)-1)
summary(merged)
```

## TF-IDF/Dictionary Approach Features
```{r}
email_corpus <- VCorpus(VectorSource(merged$content))
email_corpus <- tm_map(email_corpus, content_transformer(tolower))
email_corpus <- tm_map(email_corpus, content_transformer(gsub), pattern="\\W",replace=" ") # remove emojis
email_corpus <- tm_map(email_corpus, removeNumbers) # remove numbers
email_corpus <- tm_map(email_corpus, removeWords, stopwords()) # remove stop words
email_corpus <- tm_map(email_corpus, removePunctuation) # remove punctuation
email_corpus <- tm_map(email_corpus, stemDocument)
conv2space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
email_corpus <- tm_map(email_corpus, conv2space, "/")
email_corpus <- tm_map(email_corpus, conv2space, "@")
email_corpus <- tm_map(email_corpus, conv2space, "!")
email_corpus <- tm_map(email_corpus, stripWhitespace) # eliminate unneeded whitespace

dtm_control <- list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
email_dtm <- DocumentTermMatrix(email_corpus, control = dtm_control)

# neg score, placeholder for some other dictionary approach
tf_idf_features <- tm_term_score(email_dtm, terms_in_General_Inquirer_categories("Negativ")) 

# tf_idf_features <- tbl_df(as.matrix(email_dtm))
# nzv_columns <- nearZeroVar(tf_idf_features)
# tf_idf_features <- tf_idf_features[, -nzv_columns]
```

## Document-Topic Distributions as Features
```{r}
#need weightTf matrix, not Tf-IDF
dtm_control2 <- list(weighting = weightTf)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

dtm_control2$dictionary <- findFreqTerms(email_dtm2, lowfreq = 100)
email_dtm2 <- DocumentTermMatrix(email_corpus, control = dtm_control2)

# Remove docs which all frequencies are zeros (side effect of high frequency terms filtering during pre-processing)

row_totals <- apply(email_dtm2 , 1, sum)
email_dtm2 <- email_dtm2[apply(email_dtm2 , 1, sum) > 0,]

heldout_index <- sample(email_dtm2$nrow*0.1)

ks <- seq(5, 10, 1)

topic_models <- parSapply(cl = cl, ks, function(k, data) topicmodels::LDA(data, k = k, method = "Gibbs"), data = email_dtm2[-heldout_index,])

perplexities <- parSapply(cl = cl, topic_models, function(m, data) topicmodels::perplexity(m, data, use_theta = TRUE, estimate_theta = TRUE), data = email_dtm2[heldout_index,])

optimal_idx <- which.min(perplexities)

# get document-topic dist for all of email_dtm2 with posterior, then use those as features, add to features_df # # --> need to preserve names
docs_topics <- posterior(topic_models[[optimal_idx]], email_dtm2)
document_topic_dist <- docs_topics[["topics"]]

colnames(document_topic_dist) <- paste("Topic", colnames(document_topic_dist), sep = "_")

# Make sure to stop cluster *plus* insert serial backend
stopCluster(cl); print("Cluster stopped.")
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
```

## Feature Construction Method 3
```{r}

```

## Scale numerical variables
```{r}
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

denormalize <- function(x, min_x, max_x) { 
  return(x * (max_x - min_x) + min_x) 
}

combined_features_df <- merged[,-c(1,2,3)]
combined_features_df <- combined_features_df %>% mutate_if(is.numeric, normalize)

```

## Combine Text Features to original feature list
```{r}
colnames(combined_features_df) <- paste("Main", colnames(combined_features_df), sep = "_")
combined_features_df <- cbind(combined_features_df, tf_idf_features, document_topic_dist)

# remove features with near zero variance - Main_loan_advances and Main_director_fees
combined_features_df <- select(combined_features_df, -Main_loan_advances, -Main_director_fees)
```

## 3A - Split into training and validation and perform scaling
```{r}
train_idx <- sample(nrow(combined_features_df) * 0.60)
train_features_df <- combined_features_df[train_idx,]
test_features_df <- combined_features_df[-train_idx,]
train_poi <- as.data.frame(merged[train_idx,"poi"])
colnames(train_poi) <- c("poi")
test_poi <- as.data.frame(merged[-train_idx,"poi"])
colnames(test_poi) <- c("poi")
```

## 3B - Baseline C5.0 Model
```{r}
library(C50)

# common control for all the baseline methods in 3B and 3C
control <- trainControl(method = "cv", number = 5, sampling = "smote", classProbs = TRUE, summaryFunction = twoClassSummary)

# winnow - feature selection or not, trials - number of boosting iterations (more than 5 causing some warnings)
grid <- expand.grid(.model = "tree", .winnow = c(TRUE,FALSE), .trials = 1:5) 

set.seed(5152)
model_C50 <- train(poi ~ ., data = cbind(train_features_df, train_poi), method = "C5.0", 
                   metric = "ROC", trControl = control, 
                   preProcess = c("scale", "center"), 
                   tuneGrid = grid)

train_pred_C50 <- predict(model_C50, train_features_df) #predict
mean(train_pred_C50 == train_poi$poi) # calculate accuracy 0.893

test_pred_C50 <- predict(model_C50, test_features_df) #predict
mean(test_pred_C50 == test_poi$poi) # calculate accuracy 0.8039

# PRECISION and RECALL
test_cm_C50 <- table(predict(model_C50, test_features_df), test_poi$poi)
confusionMatrix(test_cm_C50, mode = 'everything') # acc 0.8039, pre 0.973, recall 0.8043

# AUC
test_pred_C50 <- ROCR::prediction(as.numeric(predict(model_C50, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_C50, "auc")@y.values) # 0.80 auc
```

## 3B - Baseline k-NN Model
```{r}
library(caret)
library(tidyverse)

set.seed(5152)
model_knn <- train(poi ~., data = cbind(train_features_df, train_poi), method="knn", trControl = control,
                   tuneGrid = expand.grid(k = c(3, 7, 11)),
                   preProcess = c("scale", "center"),
                   metric = "ROC")

# Training accuracy 0.82
pred_knn_tr <- predict(model_knn, newdata = train_features_df)
mean(pred_knn_tr == train_poi$poi)

# Testing accuracy 0.82
pred_knn <- predict(model_knn,newdata = test_features_df)
mean(pred_knn == test_poi$poi)
cm = confusionMatrix(pred_knn, test_poi$poi)

cm$byClass["Precision"] # 0.93
cm$byClass["Recall"] # 0.869

# AUC
test_pred_knn <- ROCR::prediction(as.numeric(pred_knn), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_knn, "auc")@y.values) # 0.634 auc

```

## 3B - Baseline RPart Model
```{r}
library(rpart)

control <- trainControl(method = "cv", number = 5, sampling = "smote", classProbs= TRUE, summaryFunction = twoClassSummary)

grid <- expand.grid(.cp = c(0.005, 0.01, 0.02, 0.05, 0.1))

set.seed(5152)
rpart_model_smote <- train(poi ~ ., data = cbind(train_features_df, train_poi), 
                           method = "rpart", metric = "ROC", 
                           trControl = control, preProcess = c("scale", "center"), 
                           tuneGrid = grid)

rpart_model_smote$bestTune

# PRECISION and RECALL
test_cm_rpart <- table(predict(rpart_model_smote, test_features_df), test_poi$poi)
confusionMatrix(test_cm_rpart, mode = 'everything') # acc 0.823, pre 0.951, recall 0.847

# AUC
test_pred_rpart <- ROCR::prediction(as.numeric(predict(rpart_model_smote, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_rpart, "auc")@y.values) # 0.723 auc

```

## 3B - Baseline NNet Model
```{r}
library(nnet)
train2 = cbind(train_features_df,train_poi)
test2 = cbind(test_features_df,test_poi)

train2[is.na(train2)] = 0
test2[is.na(test2)] = 0

# nn2 = train(poi ~., method = 'nnet', trControl = trainControl(sampling='smote'), data = train2)
# summary(nn2)
# 
# nn2_pred = predict(nn2, train_features_df)
# mean(nn2_pred == train_poi$poi)
# confusionMatrix(nn2_pred, train_poi$poi, mode = 'everything') #Training accuracy 
# # test data pred
# test_pred1 = predict(nn2, test_features_df)
# mean(test_pred1 == test_poi$poi)
# confusionMatrix(test_pred1, test_poi$poi, mode = 'everything') #Testing accuracy 

# tuning the model
# cvCtrl = trainControl(method="cv", number = 5, sampling = 'smote')

grid = expand.grid(.size = c(1,2,3), .decay = c(0, 1, 2))

set.seed(5152)
model_nn = train(poi ~., method = 'nnet', data = train2, metric = "ROC",
            trControl = control, tuneGrid = grid, preProcess = c("scale", "center"))

model_nn$bestTune
# train data pred
nn_pred_train = predict(model_nn, train2)
confusionMatrix(nn_pred_train, train_poi$poi, mode = 'everything') # training accuracy 

# test data pred 
nn_pred_test = predict(model_nn, test2)
confusionMatrix(nn_pred_test, test_poi$poi, mode = 'everything') # testing accuracy - 0.78, prec - 0.926, recall - 0.826

# AUC
test_pred_nn <- ROCR::prediction(as.numeric(predict(model_nn, test_features_df)), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_nn, "auc")@y.values) # 0.61 auc

```


## 3B - Baseline Support Vector Machine Model
```{r}
library(caret)
library(e1071)
library(DMwR)

set.seed(5152)
# control <-  trainControl(method="cv", number = 5, sampling = 'smote')- using same control as above
model_svm  <- train(factor(poi) ~., data = cbind(train_features_df, train_poi),
                    metric = "ROC", method = "svmLinear", 
                    trControl = control, preProcess = c("scale", "center"))

# Variable(s) `' constant. Cannot scale data.???

# training accuracy 0.96
pred_svm_tr <- predict(model_svm, train_features_df)
mean(pred_svm_tr == train_poi$poi)

# testing accuracy 0.84
pred_svm <- predict(model_svm, test_features_df)
mean(pred_svm == test_poi$poi)
cm = confusionMatrix(pred_svm, test_poi$poi)

cm$byClass["Precision"] # 0.952
cm$byClass["Recall"] # 0.869

# AUC
test_pred_svm <- ROCR::prediction(as.numeric(pred_svm), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_svm, "auc")@y.values) # 0.734 auc

```

## 3C - Random Forest SMOTE - improves Recall rate to 85.7%
```{r}
rf_model <- train(poi ~ ., data = cbind(train_features_df,train_poi), method = "rf", trControl = trainControl(method = "cv", number = 5))

test_cm <- table(predict(rf_model, test_features_df), test_poi$poi)
confusionMatrix(test_cm, mode = "everything")

rf_model_smote <- train(poi ~ ., data = cbind(train_features_df,train_poi), 
                        method = "rf", trControl = control, metric = "ROC")
test_cm <- table(predict(rf_model_smote, test_features_df), test_poi$poi)
confusionMatrix(test_cm, mode = "everything")

# AUC
test_pred_svm <- ROCR::prediction(as.numeric(pred_svm), as.numeric(test_poi$poi))
as.numeric(performance(test_pred_svm, "auc")@y.values) # 0.756 auc
```

## 3C - Random Forest using Ranger
```{r}
ctrl <- trainControl(method = 'cv', number = 3,verboseIter = TRUE)
ranger_mod <- train(x = train_features_df, 
                y = train_poi$poi, 
                method = "ranger",
                trControl = ctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(train_features_df)[2])),
                                      splitrule = "gini",
                                      min.node.size = 1))

train_pred <- predict(ranger_mod, train_features_df)
train_cm = as.matrix(table(Actual=train_poi$po, Predicted=train_pred))
train_cm

test_pred <- predict(ranger_mod, test_features_df)
test_cm = as.matrix(table(Actual=test_poi$poi, Predicted=test_pred))
test_cm
```

## 3C - Boosting
```{r}
# xgboost with caret train

```

## 3C - Ensemble Stacking
```{r}

library(caretEnsemble)
library(caret)

TL=list(
   m1=caretModelSpec(method='C5.0'),
   m2=caretModelSpec(method='rpart'),
   m3=caretModelSpec(method='ranger'))

folds = createFolds(train_poi$poi, k = 5)
stack_control = trainControl(method='repeatedcv', number=5, repeats=3, index=folds, savePredictions='final', classProbs=TRUE, summaryFunction=twoClassSummary)

models = caretList(poi ~ ., data=train2, metric = 'ROC', trControl=stack_control, tuneList = TL)


results = resamples(models)
summary(results)

stack.glm = caretStack(models, method="glm", metric="Accuracy", trControl=stack_control)
print(stack.glm)
  
stack_pred = predict(stack.glm, train_features_df)
confusionMatrix(stack_pred,train_poi$poi, mode = 'everything')

stack_pred_test = predict(stack.glm, test_features_df)
confusionMatrix(stack_pred_test,test_poi$poi, mode = 'everything')

```

### 3D - Features Engineering

```{r}
merged$to_poi_ratio = merged$from_poi_to_this_person / merged$to_messages
merged$from_poi_ratio = merged$from_this_person_to_poi / merged$from_messages
merged$shared_poi_ratio = merged$shared_receipt_with_poi / merged$to_messages
```

Report how much those 3 new features improve the prediction performance based on top algorithms in 3B and 3C. You don’t need to re-run all algorithms in 3B and C, but you need to report the results of at least ONE best algorithm in 3B and ONE best algorithm in 3C to show the benefit of features engineering.

### 3B and 3C models again with engineered features
```{r}

```
